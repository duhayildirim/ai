---
title: Settings
---

import { Callout } from 'nextra-theme-docs';

# Settings

All AI functions (`generateText`, `streamText`, `generateObject`, `streamObject`) support the following common settings in addition to the model and the [prompt](/docs/ai-core/prompt):

- **maxTokens** - Maximum number of tokens to generate.
- **temperature** - Temperature setting.
  The value is passed through to the provider. The range depends on the provider and model.
  It is recommended to set either `temperature` or `topP`, but not both.
- **topP** - Nucleus sampling.
  The value is passed through to the provider. The range depends on the provider and model.
  It is recommended to set either `temperature` or `topP`, but not both.
- **presencePenalty** - Presence penalty setting.
  It affects the likelihood of the model to repeat information that is already in the prompt.
  The value is passed through to the provider. The range depends on the provider and model.
- **frequencyPenalty** - Frequency penalty setting.
  It affects the likelihood of the model to repeatedly use the same words or phrases.
  The value is passed through to the provider. The range depends on the provider and model.
- **seed** - The seed (integer) to use for random sampling.
  If set and supported by the model, calls will generate deterministic results.
- **maxRetries** - Maximum number of retries. Set to 0 to disable retries. Default: 2.
- **abortSignal** - An optional abort signal that can be used to cancel the call.

<Callout type="info">
  Some providers do not support all common settings. If you use a setting with a
  provider that does not support it, a warning will be included in the AI
  function result object.
</Callout>

## Example

```ts {3-5}
const result = await experimental_generateText({
  model,
  maxTokens: 512,
  temperature: 0.3,
  maxRetries: 5,
  prompt: 'Invent a new holiday and describe its traditions.',
});
```
